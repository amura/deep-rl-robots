{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python364jvsc74a57bd03e39b0670366da3dec20bd672ea68beb42d2294e32c10d5c92639142e74d2f57",
   "display_name": "Python 3.6.4 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Exploring gym environment\n",
    "[useful ref](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/)\n",
    ">     Passenger locations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "    - 4: in taxi (Taxi is yellow)\n",
    "    Destinations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "    Actions:\n",
    "    There are 6 discrete deterministic actions:\n",
    "    - 0: move south\n",
    "    - 1: move north\n",
    "    - 2: move east\n",
    "    - 3: move west\n",
    "    - 4: pickup passenger\n",
    "    - 5: drop off passenger\n",
    "\n",
    "> Summary\n",
    "    5x5x5x4 = 500 possible states\n",
    "\n",
    "` Each step in environment returns\n",
    "    Observation\n",
    "    Reward\n",
    "    done: If step resulted in drop off / pick up -> episode\n",
    "    Info`\n",
    "\n",
    "> Blue letter : Current passenger pick up locations\n",
    "  Purple letter is current destination"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+\n|\u001b[34;1mR\u001b[0m:\u001b[43m \u001b[0m| : :G|\n| : | : : |\n| : : : : |\n| | : | : |\n|\u001b[35mY\u001b[0m| : |B: |\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.render()"
   ]
  },
  {
   "source": [
    "## States and Actions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Actions Discrete(6)\nStates Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Actions {}\".format(env.action_space))\n",
    "print(\"States {}\".format(env.observation_space))"
   ]
  },
  {
   "source": [
    "## Rendering different states\n",
    "### Random steps\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+\n|\u001b[35mR\u001b[0m: | : :G|\n| : | : : |\n| : : : : |\n| | :\u001b[43m \u001b[0m| : |\n|Y| : |\u001b[34;1mB\u001b[0m: |\n+---------+\n  (North)\n"
     ]
    }
   ],
   "source": [
    "# reset to get random initial state\n",
    "env.reset()\n",
    "result = env.step(1) # step env by 1 timestep, returns: <observation, reward, done, info>\n",
    "env.render()"
   ]
  },
  {
   "source": [
    "### Trying out a move\n",
    "> set random positions\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n+---------+\n|\u001b[35mR\u001b[0m: | : :G|\n| : | : : |\n| : : : : |\n| | :\u001b[43m \u001b[0m| : |\n|Y| : |\u001b[34;1mB\u001b[0m: |\n+---------+\n  (North)\n"
     ]
    }
   ],
   "source": [
    "env.s = env.action_space.sample()\n",
    "print(env.s)\n",
    "env.render()\n"
   ]
  },
  {
   "source": [
    "Move south and visualize state"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(452, -1, False, {'prob': 1.0})\n+---------+\n|\u001b[35mR\u001b[0m: | : :G|\n| : | : : |\n| : : : : |\n| | : | : |\n|Y| :\u001b[43m \u001b[0m|\u001b[34;1mB\u001b[0m: |\n+---------+\n  (South)\n"
     ]
    }
   ],
   "source": [
    "# set position for taxi row, col, passenger, destination\n",
    "# Location of passenger: Yellow, destination is Green, Taxi in row 0, col 1\n",
    "newState = env.step(0)\n",
    "env.s = newState\n",
    "print(env.s)\n",
    "env.render()"
   ]
  },
  {
   "source": [
    "Take random action and visualize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(111, -1, False, {'prob': 1.0})\n+---------+\n|R: | : :G|\n|\u001b[43m \u001b[0m: | : : |\n| : : : : |\n| | : | : |\n|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n+---------+\n  (West)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import random\n",
    "env.reset()\n",
    "ticks = 0\n",
    "done = False\n",
    "while not done:    \n",
    "    k = random.randint(0,3)\n",
    "    newState = env.step(k)\n",
    "    print(newState)\n",
    "    env.render()    \n",
    "    sleep(.2)\n",
    "    clear_output(wait=True)\n",
    "    ticks += 1\n",
    "    if ticks == 100:\n",
    "        done = True"
   ]
  },
  {
   "source": [
    "Reward table for the final state from above"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "111\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: [(1.0, 211, -1, False)],\n",
       " 1: [(1.0, 11, -1, False)],\n",
       " 2: [(1.0, 131, -1, False)],\n",
       " 3: [(1.0, 111, -1, False)],\n",
       " 4: [(1.0, 111, -10, False)],\n",
       " 5: [(1.0, 111, -10, False)]}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "\n",
    "print(newState[0])\n",
    "env.P[newState[0]]"
   ]
  },
  {
   "source": [
    "`{action: [(probability, nextstate, reward, done)]}`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Module 2\n",
    "\n",
    "## Q learning algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "$$\n",
    "Q(S_{t}, A_{t})  = Q(S_{t}, A_{t})+\\alpha[R_{t+1}+\\gamma*max Q(S_{t+1}, a) - Q(S_{t}, A_{t})]\n",
    "$$\n",
    "\n",
    " alpha: Step size / learning rate (0<=1)\n",
    " \n",
    "  gamma: Discount factor (0<=1)\n",
    "\n",
    "  R_t+1: Observed reward\n",
    "  \n",
    "  Q(S_t, A_t): Current Q value"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Implementing algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q table\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "# defined q table as states * actions\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.7\n",
    "epsilon = 0.1\n",
    "\n",
    "epochs, penalties, reward = 0,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished episode 49900\n"
     ]
    }
   ],
   "source": [
    "steps = 50000\n",
    "acc_reward = []\n",
    "episode_times = []\n",
    "start_time = time.time()\n",
    "for i in range(1, steps):\n",
    "    # reset the environment\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        clear_output(wait=True)\n",
    "        # explore vs exploit prob        \n",
    "        exp = random.uniform(0,1)\n",
    "        if exp < epsilon:\n",
    "            # pick from sample in action space\n",
    "            action =env.action_space.sample()\n",
    "        else:\n",
    "            # exploit best value\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        #print(\"Action:\" + action)\n",
    "        #print(next_state)\n",
    "\n",
    "        # get values for q function\n",
    "        currentQ = q_table[state, action]\n",
    "\n",
    "        #max q next state\n",
    "        maxNxt = np.max(q_table[next_state])\n",
    "\n",
    "        newQ = (1- alpha) * currentQ + alpha*(reward + gamma * maxNxt)\n",
    "        # save new q value for the given state and action\n",
    "        q_table[state, action] = newQ\n",
    "\n",
    "        # move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # check the reward\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Finished episode {i}\")\n",
    "    \n",
    "    acc_reward.append(episode_reward)\n",
    "    episode_times.append(time.time())\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Test episode\n",
    "Use of trainined agent from given a state"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "426\n+---------+\n|R: | : :\u001b[34;1mG\u001b[0m|\n| : | : : |\n| : : : : |\n| | : | : |\n|\u001b[35mY\u001b[0m|\u001b[43m \u001b[0m: |B: |\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "testState = env.reset()\n",
    "print(testState)\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished in 17\nWall time: 32.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "frames = []\n",
    "state = testState\n",
    "epochs = 0\n",
    "done = False\n",
    "while not done:\n",
    "    clear_output(wait=True)\n",
    "    # exploit best value\n",
    "    action = np.argmax(q_table[state])       \n",
    "    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    epochs += 1\n",
    "\n",
    "print(f\"Finished in {epochs}\")"
   ]
  },
  {
   "source": [
    "### Visualize the training episode"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+\n|R: | : :G|\n| : | : : |\n| : : : : |\n| | : | : |\n|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n+---------+\n  (Dropoff)\n\n"
     ]
    }
   ],
   "source": [
    "def display_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "\n",
    "        sleep(.2)\n",
    "\n",
    "display_frames(frames)"
   ]
  },
  {
   "source": [
    "### Reward accumulated per episode vs time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'acc_reward' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d2be1b8dc966>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accumulated reward vs time step\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Time step\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'acc_reward' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(acc_reward)\n",
    "plt.title(\"Accumulated reward vs time step\")\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}